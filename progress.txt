# Ralph Progress Log - Qwen3-VL Ollama Backend

Branch: ralph/qwen3-vl-ollama-backend
Started: 2026-01-20

## Codebase Patterns
- Run `npm install` before running typecheck/lint (node_modules not committed)
- Python backend files go in backend/ subdirectory
- Typecheck is `npm run typecheck`, Lint is `npm run lint`

## Context
This backend creates a local Ollama-based server to replace the existing Vercel/cloud API setup.
Backend files go in backend/ subdirectory alongside the existing React frontend.

---

## 2026-01-21 - US-001
- What was implemented: Backend directory structure with Python dependencies
- Files changed:
  - backend/requirements.txt (created with fastapi, uvicorn[standard], httpx)
  - backend/main.py (created empty)
  - backend/ollama_client.py (created empty)
  - backend/image_processor.py (created empty)
- **Learnings for future iterations:**
  - Run `npm install` first before trying to typecheck
  - Empty Python files don't affect TypeScript typecheck (expected)
---

## 2026-01-21 - US-002
- What was implemented: Base64 image decoding function in image_processor.py
- Files changed:
  - backend/image_processor.py (implemented decode_base64_image function)
- **Learnings for future iterations:**
  - Python files in backend/ don't need any special TypeScript config - they're ignored by tsc
  - Use regex pattern matching for data URL parsing to validate format and extract data in one step
  - Supported mime types: image/jpeg, image/png, image/gif, image/webp
---

## 2026-01-21 - US-003
- What was implemented: Async function to fetch images from URLs using httpx
- Files changed:
  - backend/image_processor.py (added fetch_image_from_url async function)
- **Learnings for future iterations:**
  - httpx.AsyncClient timeout parameter takes a float in seconds (e.g., 10.0 for 10 seconds)
  - Handle specific httpx exceptions: TimeoutException, HTTPStatusError, RequestError for descriptive error messages
  - Use context manager (async with) for httpx.AsyncClient to properly manage connections
---

## 2026-01-21 - US-004
- What was implemented: Ollama client for vision chat completions
- Files changed:
  - backend/ollama_client.py (implemented generate_completion async function)
- **Learnings for future iterations:**
  - Ollama API endpoint for chat is `/api/chat` at `http://localhost:11434`
  - Images are passed as base64-encoded strings in an `images` array on the user message
  - Use longer timeout (120s) for model inference since vision models can be slow
  - Response contains `message.content`, `prompt_eval_count`, and `eval_count` fields
  - httpx.ConnectError is raised automatically when Ollama server is not reachable
---

